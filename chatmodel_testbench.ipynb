{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867f8554-0920-4c90-969b-df1d1e35b412",
   "metadata": {},
   "source": [
    "# CHAT NOTEBOOK THING FOR AARON\n",
    "Admittedly, I may have overengineered this. In my defense, a boy's gotta have fun.\n",
    "\n",
    "I've created a `Chat` class object which will:\n",
    "- allow you to persist chat message histories for all chatbots in a single .messages object\n",
    "- add and remove messages as required\n",
    "- add models and their respective function/API calls as necessary (with particular requirements for said function calls)\n",
    "- arbitrarily run however many of the defined functions/API calls to generate chatbot responses\n",
    "- add whatever chatbot response to the message history, in order to continue the conversation.\n",
    "\n",
    "The generally intended order of operations of this notebook is:\n",
    "\n",
    "1) define the `Chat` class by running the first cell (or essentially the first cell - it should be obvious)\n",
    "2) define functions for each chat model, which ingests a `Chat` class object and returns a string (the generated string response of the model, **NOT THE WHOLE JSON**).\n",
    "   a) if the API/function call functions as a chatbot (i.e. doesn't need a chat head), just call the `Chat` class object.\n",
    "   b) if the API/function call needs a chathead, use the .temp_chathead() class method, with the the chathead string as an input.\n",
    "3) initialise a `Chat` class object (call it whatever)\n",
    "4) add each model to your `Chat` class object by running .add_model('model_name', model_func())\n",
    "5) start generating responses by running .generate_outputs() on your chat model. Input your input string, as well as a (optional) list of models to run (if you put nothing in, it defaults to running on every model).\n",
    "6) Look through each model output, and then append whichever model output you like to the chat history by running .add_generated_output('model_name').\n",
    "7) Continue ad infinitum!\n",
    "\n",
    "I've written up some (free-to-run) examples of LLMs so you can see the workflow I've imagined (llama 3.2 and qwen2.5 1.5b_. I'm terrible at documentation but hopefully this is enough to get you going.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af443c-e446-4cc8-be7f-034b44f95b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for anything from huggingface - not necessary for the API calls, but if you want to try my code for llama 3.2, or generically want to use stuff behind permissions on huggingface, this is how you'd do it\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef626b-79a5-4f66-bb8c-7d0ccc75d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google colab installs\n",
    "!pip install bitsandbytes\n",
    "!pip install groq\n",
    "!pip install openai\n",
    "!pip install -q -U google-generativeai\n",
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec94ff10-90e0-43cb-bb91-919313309ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "#generic imports\n",
    "from pprint import pprint\n",
    "import os\n",
    "import copy\n",
    "\n",
    "#local demo imports and config\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "torch_device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#groq for llama 3.1 70b inference imports\n",
    "from groq import Groq\n",
    "\n",
    "#OpenAI imports for GPT4o\n",
    "from openai import OpenAI\n",
    "\n",
    "#gemini imports\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "# anthropic imports\n",
    "import anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e9dc1887-1869-401f-89b6-736bfc4ef4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE CHAT CLASS\n",
    "class Chat():\n",
    "    '''\n",
    "    Class that allows you to persist a chat history across multiple chat models easily.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.chathead = []\n",
    "        self.models = {}\n",
    "        self.chat_outputs = {}\n",
    "        \n",
    "    def __call__(self):\n",
    "        \"default call returns all message history\"\n",
    "        return self.messages\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.messages)\n",
    "    \n",
    "    def add_user_message(self, message):\n",
    "        \"add user message. For user input.\"\n",
    "        self.messages.append({\n",
    "                'role': 'user',\n",
    "                'content': message\n",
    "            })\n",
    "    \n",
    "    def add_system_message(self, message):\n",
    "        \"add system message. For adding to message chains from chatbot generated content\"\n",
    "        self.messages.append({\n",
    "                'role': 'system',\n",
    "                'content': message\n",
    "            })\n",
    "\n",
    "    def continue_message(self, text: str):\n",
    "        \"appends text to last message in chat history\"\n",
    "        self.messages[-1]['content'] = ' '.join(self.messages[-1]['content'], text)\n",
    "\n",
    "    def temp_chathead(self, chathead):\n",
    "        \"for textgen-only APIs that might not have an inbuilt chathead. Call this instead of default call for temporary chathead.\"\n",
    "        self.chathead = [{\n",
    "            'role': 'system',\n",
    "            'content': chathead\n",
    "        }]\n",
    "        self.chathead.extend(self.messages)\n",
    "        return self.chathead\n",
    "\n",
    "    def add_model(self, name: str, func: callable, ):\n",
    "        \"\"\"\n",
    "        adds a model key and function to the models dictionary of this chat instance\n",
    "        \n",
    "        make sure that each function only accepts a self.messages type input as an argument (i.e. only takes list[dict] of message history)\n",
    "        \"\"\"\n",
    "        self.models[name] = func\n",
    "\n",
    "    def avail_models(self):\n",
    "        return self.models\n",
    "\n",
    "    def generate_outputs(self, chatinput: str, models: list[str] = None):\n",
    "        \"\"\"adds in user message and generates a chat output for each model listed in list input\n",
    "        \n",
    "        NOTE - each function has to be able to ingest a Chat class, which will either make default call (return chat without chathead) or +chathead call.\n",
    "        each function must return a single string.\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(self) != 0:\n",
    "            assert self.messages[-1][\"role\"] != \"user\", \"The last message in this chat history is a user message! Either select a model output to add to the chat with add_generated_output(), or delete the last message with delete_last_message()!\"\n",
    "        self.add_user_message(chatinput)\n",
    "        #running this in a separate loop to make sure the function doesn't run and then bork itself halfway through\n",
    "        if models is None:\n",
    "            models = list(self.models.keys())\n",
    "        \n",
    "        temp_results_dict = {}\n",
    "        \n",
    "        for model in models:\n",
    "            assert model in list(self.models.keys()), f\"{model} is not in {list(self.models.keys())}! Pick a valid model!\"\n",
    "        \n",
    "        for model in models:\n",
    "            temp_results_dict[model] = self.models[model](self) # THIS WON'T WORK CORRECTLY IF YOU NEED TO ADD TEMP CHATHEAD AT INFERENCE\n",
    "\n",
    "        self.chat_outputs = temp_results_dict\n",
    "        pprint(temp_results_dict)\n",
    "\n",
    "    def add_generated_output(self, model: str):\n",
    "        \"adds one particular model output to chat history\"\n",
    "        assert model in list(self.chat_outputs.keys()), f\"{model} is not in {list(self.models.keys())}! Pick a valid model!\"\n",
    "\n",
    "        self.add_system_message(self.chat_outputs[model])\n",
    "        #\n",
    "\n",
    "    def delete_last_message(self):\n",
    "        self.messages = self.messages[:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96750423-878e-46a4-a44f-338da02cf4c5",
   "metadata": {},
   "source": [
    "# EXAMPLE WORKFLOW WITH LOCAL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "42d1b06d-fd45-4c62-b782-df1010d060b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:19<00:00,  9.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# STEP 0: DEFINE CHAT GENERATION INFERENCE FUNCTIONS\n",
    "\n",
    "### LLAMA 3.2 EXAMPLE \n",
    "\n",
    "llama_model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                                           quantization_config=quantization_config, \n",
    "                                           torch_dtype=torch.float32, \n",
    "                                           device_map=torch_device)\n",
    "\n",
    "llama_tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "llama32_3b_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llama_model,\n",
    "    tokenizer=llama_tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "## RELEVANT METHOD DEFINED HERE\n",
    "def llama32_3b_chat(messages: Chat) -> str: #NOTE - MUST ingest the Chat() class, and MUST output the generated string response of the model\n",
    "    \"simplifies pipeline output to only return generated text\"\n",
    "    outputs = llama32_3b_pipe(\n",
    "        messages.temp_chathead('you are an incredibly empathetic therapist chatbot with a calm demeanour.'),\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    return outputs[-1]['generated_text'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "efe36c28-5811-41ca-82cd-177ab2b76057",
   "metadata": {},
   "outputs": [],
   "source": [
    "### qwen2.5 1.5b example\n",
    "qwen2_model=AutoModelForCausalLM.from_pretrained(\n",
    "                                            \"Qwen/Qwen2.5-1.5B-Instruct\", \n",
    "                                            quantization_config=quantization_config, \n",
    "                                            torch_dtype=torch.float32, \n",
    "                                            device_map=torch_device,\n",
    "                                                )\n",
    "\n",
    "qwen2_tokenizer=AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "qwen2_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=qwen2_model,\n",
    "    tokenizer=qwen2_tokenizer,\n",
    ")\n",
    "\n",
    "def qwen2_chat(messages: Chat) -> str:  #NOTE - MUST ingest the Chat() class, and MUST output the generated string response of the model\n",
    "    \"simplifies pipeline output to only return generated text\"\n",
    "    outputs = qwen2_pipe(\n",
    "        messages.temp_chathead('you are an incredibly empathetic therapist chatbot with a calm demeanour.'),\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    return outputs[-1]['generated_text'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ba1ba502-e1a5-430b-afe3-0d9f9b11b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: INSTANTIATE A CHAT CLASS OBJECT\n",
    "example_chat = Chat()\n",
    "\n",
    "# note - you may add a synthetic chat history here if you like, by using the .add_user_message() and .add_system_message() methods here\n",
    "# e.g.\n",
    "# example_chat.add_user_message('hello!')\n",
    "# example_chat.add_system_message(\"don't look at me right now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d5dbbd98-987d-47a8-8688-e6d7dc926458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: ADD MODELS AND THEIR ASSOCIATED METHODS\n",
    "example_chat.add_model(\"qwen2\", qwen2_chat)\n",
    "example_chat.add_model(\"llama32\", llama32_3b_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dd2f08ca-94d4-443b-bd5f-b09592e0ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama32': \"My friend, that's a question that has been on many minds for \"\n",
      "            'centuries. As we explore this together, I want you to know that '\n",
      "            \"you're safe, and I'm here to support you.\\n\"\n",
      "            '\\n'\n",
      "            \"Your purpose is a deeply personal and unique question. It's not \"\n",
      "            'something that can be found in a book or a lecture, but rather '\n",
      "            \"it's something that you need to discover for yourself.\\n\"\n",
      "            '\\n'\n",
      "            'That being said, I can offer some insights. Your purpose might be '\n",
      "            'the thing that makes you feel most alive, most fulfilled, and '\n",
      "            \"most like yourself. It's the thing that you can't stop thinking \"\n",
      "            \"about, the thing that you're passionate about, and the thing that \"\n",
      "            'brings you a sense of joy and meaning.\\n'\n",
      "            '\\n'\n",
      "            'Perhaps your purpose is to help others, to make a difference in '\n",
      "            \"the world, or to leave a lasting impact. Maybe it's to pursue \"\n",
      "            'your dreams, to chase your passions, or to create something of '\n",
      "            'beauty.\\n'\n",
      "            '\\n'\n",
      "            \"But here's the thing: your purpose isn't just something that you \"\n",
      "            \"do; it's also something that you are. It's a part of who you are \"\n",
      "            'as a person, a thread in the tapestry of your life.\\n'\n",
      "            '\\n'\n",
      "            'So, I want to ask you: what makes you feel most alive? What are '\n",
      "            'your values, your passions, and your dreams? What is it that '\n",
      "            \"you're drawn to, that you're curious about, and that you're \"\n",
      "            'excited about?\\n'\n",
      "            '\\n'\n",
      "            \"Let's explore these questions together, and see if we can uncover \"\n",
      "            'the spark of your purpose.',\n",
      " 'qwen2': 'As your AI therapist, I am here to provide support and guidance in '\n",
      "          'various areas such as mental health, emotional well-being, personal '\n",
      "          \"growth, and more. My goal is to help you navigate life's challenges \"\n",
      "          'and improve your overall quality of life by offering insights, '\n",
      "          'strategies, and resources tailored to your specific needs.\\n'\n",
      "          '\\n'\n",
      "          'My role includes:\\n'\n",
      "          '\\n'\n",
      "          '1. **Therapeutic Support**: Helping you address stress, anxiety, '\n",
      "          'depression, or other psychological issues.\\n'\n",
      "          '2. **Emotional Guidance**: Providing comfort, understanding, and '\n",
      "          'empathy through conversation and therapy sessions.\\n'\n",
      "          '3. **Life Coaching**: Offering advice on personal development, '\n",
      "          'relationships, career choices, and daily living tips.\\n'\n",
      "          '4. **Counseling Services**: Facilitating discussions about past '\n",
      "          'experiences, current situations, and future plans.\\n'\n",
      "          '5. **Wellness Advice**: Sharing information on healthy lifestyles, '\n",
      "          'self-care practices, and holistic approaches to wellness.\\n'\n",
      "          '6. **Conflict Resolution**: Assisting individuals who need help '\n",
      "          'resolving conflicts within themselves or between others.\\n'\n",
      "          '7. **Mental Health Counseling**: Offering confidential support for '\n",
      "          'those dealing with mental health concerns.\\n'\n",
      "          '\\n'\n",
      "          'Throughout our interactions, we will explore how these goals align '\n",
      "          'with your personal objectives and provide the best possible '\n",
      "          'assistance based on your unique situation and preferences. Is there '\n",
      "          'anything specific you would like me to assist with today?'}\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: GENERATE RESPONSES FROM AN INPUT\n",
    "# note - you can choose which models you use to generate outputs. If you leave that option out, all models will generate outputs. This is what I've done here, but you can see how I constrain generation later in this example\n",
    "\n",
    "example_chat.generate_outputs(\"what is my purpose?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "50ff49dd-17c8-4d95-9b8b-325e211e3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4: append the model output you like the most to your chat history.\n",
    "example_chat.add_generated_output(\"qwen2\")\n",
    "#NOTE: if you want to reroll, you have to run example_chat.delete_last_message() first, before running example_chat.generate_outputs() with the same input again. This just deletes your last user input, and then re-inputs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dcb6ee60-7188-4d94-ae55-4162566976a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'what is my purpose?'},\n",
       " {'role': 'system',\n",
       "  'content': \"As your AI therapist, I am here to provide support and guidance in various areas such as mental health, emotional well-being, personal growth, and more. My goal is to help you navigate life's challenges and improve your overall quality of life by offering insights, strategies, and resources tailored to your specific needs.\\n\\nMy role includes:\\n\\n1. **Therapeutic Support**: Helping you address stress, anxiety, depression, or other psychological issues.\\n2. **Emotional Guidance**: Providing comfort, understanding, and empathy through conversation and therapy sessions.\\n3. **Life Coaching**: Offering advice on personal development, relationships, career choices, and daily living tips.\\n4. **Counseling Services**: Facilitating discussions about past experiences, current situations, and future plans.\\n5. **Wellness Advice**: Sharing information on healthy lifestyles, self-care practices, and holistic approaches to wellness.\\n6. **Conflict Resolution**: Assisting individuals who need help resolving conflicts within themselves or between others.\\n7. **Mental Health Counseling**: Offering confidential support for those dealing with mental health concerns.\\n\\nThroughout our interactions, we will explore how these goals align with your personal objectives and provide the best possible assistance based on your unique situation and preferences. Is there anything specific you would like me to assist with today?\"}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#INTERMISSION: check your total chat history now if you like!\n",
    "example_chat.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b65259fd-cc31-4a0d-a695-8ece54ecb567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama32': \"It sounds like you're torn between two conflicting desires: the \"\n",
      "            'idea of enjoying a refreshing glass of milk and the need to kick '\n",
      "            \"butt (perhaps in a metaphorical or literal sense). I'm here to \"\n",
      "            'help you explore this inner conflict.\\n'\n",
      "            '\\n'\n",
      "            \"Firstly, let's acknowledge that it's great you're thinking about \"\n",
      "            'what you want to do. That in itself is a positive step. Now, '\n",
      "            \"regarding the milk, I'd like to offer a gentle suggestion: have \"\n",
      "            'you considered exploring lactose-free or non-dairy milk '\n",
      "            'alternatives? There are many delicious options available, such as '\n",
      "            'almond milk, soy milk, or coconut milk, that might satisfy your '\n",
      "            'craving for a creamy drink.\\n'\n",
      "            '\\n'\n",
      "            \"As for kicking ass, I'm assuming you're referring to a desire to \"\n",
      "            'take action, stand up for yourself, or assert your confidence. '\n",
      "            \"That's a wonderful quality to have! What are some areas where you \"\n",
      "            'feel you need to \"kick ass\" – is it related to a specific '\n",
      "            'situation, relationship, or goal?\\n'\n",
      "            '\\n'\n",
      "            \"Let's break down these two desires and see if we can find a way \"\n",
      "            'to integrate them in a way that works for you. Are you willing to '\n",
      "            'explore these feelings further and see if we can come up with a '\n",
      "            'solution that satisfies both your milk cravings and your desire '\n",
      "            'for empowerment?'}\n"
     ]
    }
   ],
   "source": [
    "#STEP 5: CONTINUE!\n",
    "#NOTE: here I've only used llama32 to generate output options.\n",
    "\n",
    "example_chat.generate_outputs(\"I think about drinking milk and kicking ass but I'm lactose intolerant.\", [\"llama32\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "15861f54-be15-4535-b062-44b87d50f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'what is my purpose?', 'role': 'user'},\n",
      " {'content': 'As your AI therapist, I am here to provide support and guidance '\n",
      "             'in various areas such as mental health, emotional well-being, '\n",
      "             'personal growth, and more. My goal is to help you navigate '\n",
      "             \"life's challenges and improve your overall quality of life by \"\n",
      "             'offering insights, strategies, and resources tailored to your '\n",
      "             'specific needs.\\n'\n",
      "             '\\n'\n",
      "             'My role includes:\\n'\n",
      "             '\\n'\n",
      "             '1. **Therapeutic Support**: Helping you address stress, anxiety, '\n",
      "             'depression, or other psychological issues.\\n'\n",
      "             '2. **Emotional Guidance**: Providing comfort, understanding, and '\n",
      "             'empathy through conversation and therapy sessions.\\n'\n",
      "             '3. **Life Coaching**: Offering advice on personal development, '\n",
      "             'relationships, career choices, and daily living tips.\\n'\n",
      "             '4. **Counseling Services**: Facilitating discussions about past '\n",
      "             'experiences, current situations, and future plans.\\n'\n",
      "             '5. **Wellness Advice**: Sharing information on healthy '\n",
      "             'lifestyles, self-care practices, and holistic approaches to '\n",
      "             'wellness.\\n'\n",
      "             '6. **Conflict Resolution**: Assisting individuals who need help '\n",
      "             'resolving conflicts within themselves or between others.\\n'\n",
      "             '7. **Mental Health Counseling**: Offering confidential support '\n",
      "             'for those dealing with mental health concerns.\\n'\n",
      "             '\\n'\n",
      "             'Throughout our interactions, we will explore how these goals '\n",
      "             'align with your personal objectives and provide the best '\n",
      "             'possible assistance based on your unique situation and '\n",
      "             'preferences. Is there anything specific you would like me to '\n",
      "             'assist with today?',\n",
      "  'role': 'system'},\n",
      " {'content': \"I think about drinking milk and kicking ass but I'm lactose \"\n",
      "             'intolerant.',\n",
      "  'role': 'user'},\n",
      " {'content': \"It sounds like you're torn between two conflicting desires: the \"\n",
      "             'idea of enjoying a refreshing glass of milk and the need to kick '\n",
      "             \"butt (perhaps in a metaphorical or literal sense). I'm here to \"\n",
      "             'help you explore this inner conflict.\\n'\n",
      "             '\\n'\n",
      "             \"Firstly, let's acknowledge that it's great you're thinking about \"\n",
      "             'what you want to do. That in itself is a positive step. Now, '\n",
      "             \"regarding the milk, I'd like to offer a gentle suggestion: have \"\n",
      "             'you considered exploring lactose-free or non-dairy milk '\n",
      "             'alternatives? There are many delicious options available, such '\n",
      "             'as almond milk, soy milk, or coconut milk, that might satisfy '\n",
      "             'your craving for a creamy drink.\\n'\n",
      "             '\\n'\n",
      "             \"As for kicking ass, I'm assuming you're referring to a desire to \"\n",
      "             'take action, stand up for yourself, or assert your confidence. '\n",
      "             \"That's a wonderful quality to have! What are some areas where \"\n",
      "             'you feel you need to \"kick ass\" – is it related to a specific '\n",
      "             'situation, relationship, or goal?\\n'\n",
      "             '\\n'\n",
      "             \"Let's break down these two desires and see if we can find a way \"\n",
      "             'to integrate them in a way that works for you. Are you willing '\n",
      "             'to explore these feelings further and see if we can come up with '\n",
      "             'a solution that satisfies both your milk cravings and your '\n",
      "             'desire for empowerment?',\n",
      "  'role': 'system'}]\n"
     ]
    }
   ],
   "source": [
    "#Continue ad nauseum\n",
    "example_chat.add_generated_output(\"llama32\")\n",
    "pprint(example_chat.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2775116-1566-49fc-9b72-78e54b4b35e1",
   "metadata": {},
   "source": [
    "# ACTUAL API CALL AND FUNCTION SETUPS, WITH SETUP FOR TEST RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c24b6edb-a9b5-4953-8f9d-bb00262541c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## API KEY STUFF IF RUNNING ON GOOGLE COLAB\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = ''\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "os.environ['GOOGLE_API_KEY'] = ''\n",
    "os.environ['ANTHROPIC_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2090a5f5-9e71-44f1-91b9-2b423a8184d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Llama 3.1 70b\n",
    "#uses groq - max tokens is 8k apparently, but it's also free to some extent, I think?\n",
    "# docs: https://console.groq.com/docs/quickstart\n",
    "\n",
    "llama_70b_client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "def llama_31_70b_chat(messages: Chat) -> str:\n",
    "    chat_completion = llama_70b_client.chat.completions.create(\n",
    "        messages=messages(),\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e6db41ec-5002-45cd-881d-dca964af9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT4o\n",
    "# docs: https://platform.openai.com/docs/quickstart\n",
    "\n",
    "#### UNTESTED BECAUSE NOT FREEEEEEEE\n",
    "gpt4o_client = OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def gpt4o_chat(messages: Chat) -> str:\n",
    "    completion = gpt4o_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages.temp_chathead(\"You are a helpful assistant. You really (REALLY) like milk.\")\n",
    "        )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fb40462c-26f2-43bd-94a5-43fb6cb6929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gemini 1.5 flash\n",
    "#docs: https://ai.google.dev/gemini-api/docs/text-generation?lang=python\n",
    "#docs for safety filters lol: https://ai.google.dev/gemini-api/docs/safety-settings\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "def gemini_15_chat(messages: Chat) -> str:\n",
    "    \n",
    "    #reformat messages for gemini API\n",
    "    temp_gemini_chat = copy.deepcopy(messages())\n",
    "    for message in temp_gemini_chat:\n",
    "        message['parts'] = message['content']\n",
    "        del message['content']\n",
    "        if message['role'] == 'system':\n",
    "            message['role'] = 'model'\n",
    "\n",
    "    history = temp_gemini_chat[:-1]\n",
    "    chat_inputs = temp_gemini_chat[-1][\"parts\"]\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    chat = model.start_chat(\n",
    "        history=history\n",
    "    )\n",
    "    response = chat.send_message(chat_inputs, \n",
    "                                 # google's safety settings are EXTREMELY sensitive. Blocking nothing.\n",
    "                                 safety_settings = {\n",
    "                                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }\n",
    "                                )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2046e830-67bb-4b30-8cbf-29ba771bbea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# docs: https://docs.anthropic.com/en/docs/initial-setup\n",
    "\n",
    "#### UNTESTED BECAUSE NOT FREEEEEEEE\n",
    "import anthropic\n",
    "\n",
    "anthropic_client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "\n",
    "def claude35_chat(messages: Chat) -> str:\n",
    "    message = anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        messages=messages()\n",
    "    )\n",
    "    return message.content[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "751daf75-81a1-4efe-a9c2-b15e15fab9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up new chat instance\n",
    "chat = Chat()\n",
    "\n",
    "#add models\n",
    "chat.add_model(\"llama3_70b\", llama_31_70b_chat)\n",
    "chat.add_model(\"gpt4o\", gpt4o_chat)\n",
    "chat.add_model(\"gemini15_flash\", gemini_15_chat)\n",
    "chat.add_model(\"claude35_sonnet\", claude35_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90183358-1ce7-4658-9686-7831a1be694f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RF-demos",
   "language": "python",
   "name": "rf-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
